* KITTI Road Sementic Segmentation

The problem is to segment driving roads from others

** Dataset
- [[http://www.cvlibs.net/datasets/kitti/eval_road.php][KITTI Road/Lane Detection Dataset]]

Sample input and ground truth images

#+BEGIN_EXPORT html
<div>
<img src="./assets/um_000000.png" alt="road data" width="49%"/>
<img src="./assets/um_lane_000000.png" alt="ground truth" width="49%"/>
</div>
#+END_EXPORT

** Results

#+BEGIN_EXPORT html
<div>
<img src="./runs/1512199296.4987557/um_000000.png" alt="prediction data" width="33%" />
<img src="./runs/1512199296.4987557/um_000010.png" alt="prediction data" width="33%" />
<img src="./runs/1512199296.4987557/um_000020.png" alt="prediction data" width="33%" />

<img src="./runs/1512199296.4987557/um_000030.png" alt="prediction data" width="33%" />
<img src="./runs/1512199296.4987557/um_000040.png" alt="prediction data" width="33%" />
<img src="./runs/1512199296.4987557/um_000050.png" alt="prediction data" width="33%" />

<img src="./runs/1512199296.4987557/um_000060.png" alt="prediction data" width="33%" />
<img src="./runs/1512199296.4987557/um_000070.png" alt="prediction data" width="33%" />
<img src="./runs/1512199296.4987557/um_000080.png" alt="prediction data" width="33%" />
</div>
#+END_EXPORT


** Method
- Pretrained VGG16
- Skip Connection
- Fully Convolution Network

*** VGG16

First, I used the pre-trained VGG16 network

#+BEGIN_SRC python :exports code
tf.saved_model.loader.load(sess, [vgg_tag], vgg_path)
#+END_SRC


*** Skip Connection

Skip connection greatly improves the performance of network.

First, run 1x1 convolution and then followed by =conv_transpose= layer

#+BEGIN_SRC python :exports code
  # layer7 -> Conv 1x1 -> ConvT
  with tf.variable_scope("layer7_deconv"):
      conv_1x1 = run_conv_1x1(vgg_layer7_out)
      output = run_conv_transpose(conv_1x1, 4, 2)

  # layer4 -> Conv 1x1 -> ConvT
  with tf.variable_scope("layer4_deconv"):
      conv_1x1 = run_conv_1x1(vgg_layer4_out)
      merged = tf.add(output, conv_1x1)
      output = run_conv_transpose(merged, 4, 2)

  # layer3 -> Conv 1x1 -> ConvT
  with tf.variable_scope("layer3_deconv"):
      conv_1x1 = run_conv_1x1(vgg_layer3_out)
      merged = tf.add(output, conv_1x1)
      output = run_conv_transpose(merged, 16, 8, name="nn_last_layer")

  return output
#+END_SRC


*** Fully Convolution Network

After all, we will need a fully convolution network because the label is the same size as the input image.

In this example, only 2 labels(road or not road) exist. Therefore, output shape is =(?, 160, 576, 2)=.


** Hyperparameters and notes

*** Xavier Initialization
I tried xavier, he, random initializers, but xavier works the best in this example based on random search.

For the code, I used =tf.contrib.layers.variance_scaling_initializer= because it's the most versatile.

#+BEGIN_SRC python :exports code
    initializers = {"factor": 1.0, "mode": 'FAN_AVG', "uniform": True}

    def run_conv_1x1(input_):
        """Returns 1x1 Convolution"""
        return tf.layers.conv2d(
            input_,
            num_classes,
            kernel_size=1,
            padding="same",
            kernel_initializer=tf.contrib.layers.variance_scaling_initializer(
                **initializers),
            kernel_regularizer=tf.contrib.layers.l2_regularizer(reg))
#+END_SRC

*** L2 Regularizer

I usually tend to add a tiny bit of regularization terms to the kernel. So I added

#+BEGIN_SRC python :exports code
  reg = 1e-3
  kernel_regularizer=tf.contrib.layers.l2_regularizer(reg)
#+END_SRC


*** AdamOptimizer

Learning rate won't matter much because of the characteristics of =AdamOptimizer=. However, the default setting for Adam optimizer is not really great (it's even written on the [[https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/train/AdamOptimizer#__init__][official documentation]].)

Note that the epsilon is 1.0 (it's important!)
#+BEGIN_SRC python :exports code
  optim = tf.train.AdamOptimizer(learning_rate, epsilon=1.0)
  train_op = optim.minimize(xentroy_loss, global_step=global_step)
#+END_SRC

*** Saver and Builder

=tf.Saver= is the old way of saving models in Tensorflow. And [[https://www.tensorflow.org/programmers_guide/saved_model#overview_of_saving_and_restoring_models][SavedModel API]] is the new way of saving models. It is great but it's still a wrapper around =tf.Saver=.
I can see why it's beneficial, but it tends to create more unnecessary files.

**** Saver

#+BEGIN_SRC python :exports code
  saver = tf.train.Saver()

  # when loading from checkpoint
  last_checkpoint = tf.train.latest_checkpoint(FLAGS.checkpoint)

  if saver and last_checkpoint:
      saver.restore(sess, last_checkpoint)
  else:
      # if no checkpoint initialize
      sess.run(tf.global_variables_initializer())

  # when save
  saver.save(sess, os.path.join(FLAGS.checkpoint, "model.ckpt"))
#+END_SRC


**** Builder
When using =Builder=, you should be careful if the directory already exists, it will result in an error.
#+BEGIN_SRC python :exports code
  builder = tf.saved_model.builder.SavedModelBuilder("builder")

  with tf.Session(graph=tf.Graph()) as sess:
      builder.add_meta_graph_and_variables(sess, ["VGG16_SEMENTIC"])
      builder.save()

#+END_SRC

When loading from builder, it's nice and clean.

#+BEGIN_SRC python :exports code
tags = ["VGG16_SEMENTIC"]
tf.saved_model.loader.load(sess, tags, builder_path)
#+END_SRC
